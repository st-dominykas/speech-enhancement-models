{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import *\n",
    "from data import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from scipy.signal import wiener\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics.audio.pesq import PerceptualEvaluationSpeechQuality\n",
    "from torchmetrics.functional.audio.stoi import short_time_objective_intelligibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTLN = Pytorch_DTLN(frame_len=1536, \n",
    "                         frame_hop=384, \n",
    "                         dropout=0.3,\n",
    "                         encoder_size=1024,\n",
    "                         hidden_size=512,\n",
    "                         LSTM_size=2)\n",
    "DTLN.load_state_dict(torch.load('./48khz_parameter_epoch128.pth',map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim(signal_1, signal_2):\n",
    "    mu_1 = signal_1.mean()\n",
    "    mu_2 = signal_2.mean()\n",
    "    var_1 = signal_1.var()\n",
    "    var_2 = signal_2.var()\n",
    "    cov_1_2 = np.cov([signal_1, signal_2])[1][0]\n",
    "    L = abs(max(max(signal_1),max(signal_2)) -min(min(signal_1),min(signal_2)))\n",
    "    k_1 = 0.01\n",
    "    k_2 = 0.03\n",
    "    c_1 = (k_1 * L)**2\n",
    "    c_2 = (k_2 * L)**2\n",
    "\n",
    "    l = (2*mu_1*mu_2 + c_1)/(mu_1**2 + mu_2**2 + c_1)\n",
    "    c = (2*(var_1**(1/2))*(var_2**(1/2)) + c_2)/(var_1 + var_2 + c_2)\n",
    "    s = (cov_1_2 + c_2/2)/((var_1**(1/2))*(var_2**(1/2))+c_2/2)\n",
    "\n",
    "    return l*c*s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_16khz(clean_path, noisy_path):\n",
    "    # cut signal into frames of length, pass them to model and glue it back\n",
    "    clean_signal, _ = torchaudio.load(clean_path)\n",
    "    if _ != 16000:\n",
    "        clean_signal = torchaudio.transforms.Resample(_,16000)(clean_signal).flatten()\n",
    "    else:\n",
    "        clean_signal = clean_signal.flatten()\n",
    "    noisy_signal, _ = torchaudio.load(noisy_path)\n",
    "    if _ != 16000:\n",
    "        noisy_signal = torchaudio.transforms.Resample(_,16000)(noisy_signal).flatten()\n",
    "\n",
    "    # pad signal with zeros to have full frames to cut\n",
    "    pad_size = (noisy_signal.shape[0]//2**15 + 1)*(2**15) - noisy_signal.shape[0]\n",
    "    noisy_padded = F.pad(input=noisy_signal, pad=(0,pad_size), mode='constant', value=0)\n",
    "\n",
    "    # crop signal and process it piece by piece\n",
    "    denoised_signal = torch.tensor([])\n",
    "    for frame in np.arange(0, (noisy_padded.shape[0]//2**15)*(2**15), step = 2**15):\n",
    "        if denoised_signal.shape[0]==0:\n",
    "            denoised_signal = DTLN(noisy_padded[frame:frame+2**15].unsqueeze(0))[0]\n",
    "        else:\n",
    "            denoised_signal = torch.cat((denoised_signal,DTLN(noisy_padded[frame:frame+2**15].unsqueeze(0))[0]))\n",
    "\n",
    "    denoised_signal = denoised_signal[:noisy_signal.shape[0]]    \n",
    "\n",
    "    # PESQ\n",
    "    wb_pesq = PerceptualEvaluationSpeechQuality(16000, 'wb')\n",
    "    denoised_pesq = wb_pesq(denoised_signal, clean_signal)\n",
    "\n",
    "    # STOI\n",
    "    denoised_stoi = short_time_objective_intelligibility(denoised_signal, clean_signal, 16000)\n",
    "\n",
    "    # SSIM\n",
    "    denoised_ssim = ssim(clean_signal.detach().numpy(), denoised_signal.detach().numpy())\n",
    "    \n",
    "    return [denoised_pesq.item(), \n",
    "            denoised_stoi.item(), \n",
    "            denoised_ssim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_48khz(clean_path, noisy_path):\n",
    "    # cut signal into frames of length, pass them to model and glue it back\n",
    "    clean_signal, _ = torchaudio.load(clean_path)\n",
    "    if _ != 48000:\n",
    "        clean_signal = torchaudio.transforms.Resample(_,48000)(clean_signal).flatten()\n",
    "    else:\n",
    "        clean_signal = clean_signal.flatten()\n",
    "    noisy_signal, _ = torchaudio.load(noisy_path)\n",
    "    if _ != 48000:\n",
    "        noisy_signal = torchaudio.transforms.Resample(_,48000)(noisy_signal).flatten()\n",
    "    else:\n",
    "        noisy_signal = noisy_signal.flatten()\n",
    "\n",
    "    # pad signal with zeros to have full frames to cut\n",
    "    pad_size = (noisy_signal.shape[0]//2**15 + 1)*(2**15) - noisy_signal.shape[0]\n",
    "    noisy_padded = F.pad(input=noisy_signal, pad=(0,pad_size), mode='constant', value=0)\n",
    "\n",
    "    # crop signal and process it piece by piece\n",
    "    denoised_signal = torch.tensor([])\n",
    "    for frame in np.arange(0, (noisy_padded.shape[0]//2**15)*(2**15), step = 2**15):\n",
    "        if denoised_signal.shape[0]==0:\n",
    "            denoised_signal = DTLN(noisy_padded[frame:frame+2**15].unsqueeze(0))[0]\n",
    "        else:\n",
    "            denoised_signal = torch.cat((denoised_signal,DTLN(noisy_padded[frame:frame+2**15].unsqueeze(0))[0]))\n",
    "\n",
    "    denoised_signal = denoised_signal[:noisy_signal.shape[0]]    \n",
    "\n",
    "    # PESQ\n",
    "    wb_pesq = PerceptualEvaluationSpeechQuality(16000, 'wb')\n",
    "    denoised_pesq = wb_pesq(torchaudio.transforms.Resample(_,16000)(denoised_signal).flatten(), \n",
    "                            torchaudio.transforms.Resample(_,16000)(clean_signal).flatten())\n",
    "\n",
    "    # STOI\n",
    "    denoised_stoi = short_time_objective_intelligibility(denoised_signal, clean_signal, 48000)\n",
    "\n",
    "    # SSIM\n",
    "    denoised_ssim = ssim(clean_signal.detach().numpy(), denoised_signal.detach().numpy())\n",
    "    \n",
    "    return [denoised_pesq.item(), \n",
    "            denoised_stoi.item(), \n",
    "            denoised_ssim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16khz\n",
    "clean_path = ''\n",
    "noisy_path = ''\n",
    "pesq, stoi, ssim = calculate_metrics_16khz(clean_path, noisy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 48khz\n",
    "clean_path = ''\n",
    "noisy_path = ''\n",
    "pesq, stoi, ssim = calculate_metrics_48khz(clean_path, noisy_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (tags/v3.8.8:024d805, Feb 19 2021, 13:18:16) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10570769181f3bef521e17ca461192a1c83fd1537522c84236007b668e38ca34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
